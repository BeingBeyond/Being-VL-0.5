<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Being-VL-0.5: A framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Accepted by ICCV 2025.">
  <meta property="og:title" content="Unified Multimodal Understanding via Byte-Pair Visual Encoding"/>
  <meta property="og:description" content="Being-VL-0.5: A framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Accepted by ICCV 2025."/>
  <meta property="og:url" content="https://beingbeyond.github.io/Being-VL-0.5"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Unified Multimodal Understanding via Byte-Pair Visual Encoding">
  <meta name="twitter:description" content="Being-VL-0.5: A framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Accepted by ICCV 2025.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="multimodal learning, visual language models, byte-pair encoding, visual tokenization, MLLM, transformer, computer vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Unified Multimodal Understanding via Byte-Pair Visual Encoding</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <figure class="image">
            <img src="static/images/being-vl-05.png" alt="Being-VL-0.5" style="max-width: 36%; height: auto; margin: 0 auto; display: block;"/>
            </figure>
            <br>
            <h1 class="title is-1 publication-title">Unified Multimodal Understanding via Byte-Pair Visual Encoding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zhangwp.com/" target="_blank">Wanpeng Zhang</a><sup>1,4</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=Ra-5oD8AAAAJ" target="_blank">Yicheng Feng</a><sup>1,4</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=TwuNaTYAAAAJ" target="_blank">Hao Luo</a><sup>1,4</sup>,</span>
                  <span class="author-block">
                    <a href="https://williamium3000.github.io/" target="_blank">Yijiang Li</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://yuezih.github.io/" target="_blank">Zihao Yue</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://zhengsipeng.github.io/" target="_blank">Sipeng Zheng</a><sup>4</sup>,</span>
                  <span class="author-block">
                    <a href="https://z0ngqing.github.io/" target="_blank">Zongqing Lu</a><sup>1,4†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Peking University &nbsp;&nbsp; <sup>2</sup>UC San Diego &nbsp;&nbsp; <sup>3</sup>Renmin University of China &nbsp;&nbsp; <sup>4</sup>BeingBeyond</span>
                    <br>
                    <span style="font-weight: bold;color:rgb(247, 142, 22);">ICCV 2025</span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2506.23639" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/beingbeyond/Being-VL-0.5" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.23639" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
            <br>
            <figure class="image">
            <img src="static/images/framework.png" alt="Framework Overview" style="max-width: 88%; height: auto; margin: 0 auto; display: block;"/>
            </figure>
            <br>
            <div class="content has-text-justified">
              <p>
                Our framework operates in two main phases: vocabulary construction and application. The vocabulary construction phase uses a priority-guided encoding scheme that considers both co-occurrence frequency and spatial consistency of visual patterns. This creates an extended vocabulary that effectively captures meaningful visual structures beyond simple frequency-based methods. The application phase integrates visual tokens with text tokens to form unified sequences for multimodal understanding.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Encoding Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Priority-Guided Encoding</h2>
        <div class="content has-text-justified">
          <figure class="image">
          <img src="static/images/priority-encoding.png" alt="Priority-Guided Encoding" style="max-width: 100%; height: auto; margin: 0 auto; display: block;"/>
          </figure>
          <p>
          Based on the theoretical guidance from our previous paper <a href="https://openreview.net/pdf?id=3TnLGGHhNx" target="_blank">From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities</a>, we designed a priority-guided encoding mechanism: 
          </p>

          <p>
          <li><b>Co-occurrence frequency</b>: Statistics of the spatial adjacency of VQ tokens in the data.</li>
          <li><b>Spatial consistency</b>: Evaluating which token pairs maintain stable spatial relationships across different images.</li> 
          </p>

          <p>
          By integrating these two factors, our BPE tokenizer can combine frequently co-occurring and spatially consistent VQ tokens into new visual tokens, thereby encoding structured information about images at the token level. This design directly corresponds to our theoretical finding—injecting structured priors through tokenization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Multi-stage Training Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Expanding & Training</h2>
        <div class="content has-text-justified">
          <figure class="image">
          <img src="static/images/data-composition.png" alt="Multi-stage Training Strategy" style="max-width: 70%; height: auto; margin: 0 auto; display: block;"/>
          </figure>
          <p>
          We first use VQ-GAN to quantize images into discrete token sequences. Unlike MLLMs that directly use these discrete visual tokens, we believe that these initial tokens only capture very local visual information and lack high-level semantic structure. This is precisely our motivation to further expand embeddings and adopt visual BPE tokens.
          </p>
          <p>
          Visual BPE creates hierarchical representations, where encoded tokens progressively capture visual patterns from simple to complex. Based on this characteristic, we designed a matching learning curriculum: (1) Foundational Stage: Establish basic visual-language alignment; (2) Perceptual Stage: Learn detailed visual attributes; (3) Reasoning Stage: Develop complex visual reasoning capabilities; (4) Instruction Stage: Optimize task execution capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Benchmark Results Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <figure class="image">
          <img src="static/images/tab-eval-main.png" alt="Benchmark Results Table" style="max-width: 100%; height: auto; margin: 0 auto; display: block;"/>
        </figure>
        <br>
        <div class="content has-text-justified">
        <p>
          Our framework achieves competitive performance across multiple vision-language benchmarks, effectively narrowing the gap between discrete token-based and continuous embedding-based models, outperforming previous discrete token approaches while maintaining the advantages of unified token representation.
        </p>
        </div>
        <figure class="image">
          <img src="static/images/tab-ablation.png" alt="Ablation Table" style="max-width: 100%; height: auto; margin: 0 auto; display: block;"/>
        </figure>
        <br>
        <div class="content has-text-justified">
        <p>
          We conduct ablation studies to investigate the impact of the multi-stage training. The result shows that our standard approach achieves clear improvements over the single-stage training, demonstrating the effectiveness of the overall strategy.
        </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Example Cases</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/case1.png" alt="Case 1" style="max-width: 80%; height: auto; margin: 0 auto; display: block;"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/case2.png" alt="Case 2" style="max-width: 80%; height: auto; margin: 0 auto; display: block;"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/case3.png" alt="Case 3" style="max-width: 80%; height: auto; margin: 0 auto; display: block;"/>
     </div>
  </div>
</div>
</div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <p><b>Being-VL-0.5</b></p>
<pre><code>@inproceedings{zhang2025beingvl05,
  title={Unified Multimodal Understanding via Byte-Pair Visual Encoding},
  author={Wanpeng Zhang and Yicheng Feng and Hao Luo and Yijiang Li and Zihao Yue and Sipeng Zheng and Zongqing Lu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}</code></pre>
      <p><b>Being-VL-0</b></p>
<pre><code>@inproceedings{zhang2025beingvl0,
  title={From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities},
  author={Wanpeng Zhang and Zilong Xie and Yicheng Feng and Yijiang Li and Xingrun Xing and Sipeng Zheng and Zongqing Lu},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=3TnLGGHhNx}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website template is licensed under a <a rel="license"
                                                                     href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a> and adapted from source at <a
                            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://umi-on-legs.github.io/">UMI</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
